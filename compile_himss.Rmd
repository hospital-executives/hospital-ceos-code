---
title: "compile_himss"
output: html_document
date: "2024-04-02"
---

### Script Overview
Re-build Maggie's Stata code to clean and compile each year's HIMSS survey. Focus on 2005 - 2017. Recreate healthcare_execs_0517, but in R.

## Set-up

```{r manual setup user entry req, include=FALSE}
#User entry required!

#clear environment
rm(list = ls())

# *** IMPORTING RAW HIMS ***
# Define the range of years to import
year_range <- 2005:2017 

# Specify the prefixes of files you want to import
file_prefixes <- c("Contact_", "HAEntityContact_","HAEntity_", "ContactType_","ContactSource_")

# *** IMPORTING AHA DATA ***
file_name_aha <- c("aha-raw-2000-20.dta")

columns_to_keep <- c( 
  "ID", 
  "YEAR", 
  "MNAME", 
  "HOSPN", 
  "mcrnum", 
  "CBSANAME", 
  "HRRNAME", 
  "HSANAME", 
  "MADMIN", 
  "SYSID", 
  "SYSNAME", 
  "CNTRL", 
  "SERV", 
  "HOSPBD", 
  "BDTOT", 
  "ADMTOT", 
  "IPDTOT", 
  "MCRDC", 
  "MCRIPD", 
  "MCDDC", 
  "MCDIPD", 
  "BIRTHS", 
  "FTEMD", 
  "FTERN", 
  "FTE",
  "LAT",
  "LONG"
   ) # Replace with your desired column names

# *** IMPORTING AHA<>MCR CROSSWALK ***
file_name_aha_mcr_xwalk <- c("hospital_ownership.dta")

```


```{r auto setup, include=FALSE}
# NO manual entry required. This automatically loads necessary libraries and detects file paths. 
knitr::opts_chunk$set(echo = TRUE)

# Source the configuration script
# Check if rstudioapi is installed and install if necessary
if (!requireNamespace("rstudioapi", quietly = TRUE)) {
  install.packages("rstudioapi")
}

library(rstudioapi)

# Detect the path to the current script's directory dynamically
script_directory <- dirname(rstudioapi::getActiveDocumentContext()$path)

# Construct the path to the config.R file
config_path <- file.path(script_directory, "config.R")

# Source the config file dynamically
source(config_path)

#Now all necessary libraries are loaded, and file paths are set

#clean up
rm(script_directory, config_path)
```

## Pull in raw, HIMSS files

```{r import all original himss}
#a separate bash script extracted the original Access DB tables as CSVs
#these are loaded to DropBox, we only want a subset (defined in file_prefixes)

folders <- list.dirs(path = raw_data, full.names = TRUE, recursive = FALSE)

# Loop through each file prefix
for (prefix in file_prefixes) {
  temp_dfs <- list()  # Temporary list to store dataframes for this prefix
  
  # Loop through each year
  for (year in year_range) {
    file_path <- paste0(prefix, year, ".csv")  # Construct the file name
    full_path <- file.path(raw_data, as.character(year), file_path)  # Full file path
    
  #Check if the file exists, import
    if (file.exists(full_path)) {
       #Importing all as character. 
       #There are some differences within year 
       #(e.g. most phone numbers will be ##########, but some will be ###-###-####. 
       #Without converting to character, we see data loss when the mismatched value is not imported 
       #e.g. ###-###-#### would be NA
      df <- read_csv(full_path, show_col_types = FALSE, col_types = cols(.default = col_character())) %>%
        # Add a year value and and make all vars lowercase
        mutate(year = year) %>%  # Add the year as a new column
        rename_with(tolower)

      # In some years the same fields data storage type changed. Dynamically adjust data types for combination: Convert numeric and logical columns to character.
      # df <- df %>%
      #   mutate(across(where(is.numeric), ~if_else(is.na(.), as.character(.), as.character(.)))) %>%
      #   mutate(across(where(is.logical), as.character))

      temp_dfs[[length(temp_dfs) + 1]] <- df
    }
  }
  
  # Combine all dataframes, now with consistent data types
  combined_df <- bind_rows(temp_dfs)
  
  # Dynamically assign the combined dataframe to a variable in the global environment
  var_name <- tolower(str_remove(prefix, "_$"))
  assign(var_name, combined_df, envir = .GlobalEnv)
}

# Clean up temporary variables
rm(list = c("combined_df", "df", "temp_dfs","var_name","year", "year_range"))
```

```{r calculate original import misses}
#In original loop, mismatched col values (within a given year) were being dropped. this section calculates the total impact of that.

# Define the range of years to import
year_range <- 2005:2017 

# Specify the prefixes of files you want to import
file_prefixes <- c("Contact_", "HAEntityContact_", "HAEntity_", "ContactType_", "ContactSource_")

# Initialize a list to store all parsing issues
all_parsing_issues <- list()
# Initialize a variable to count the total number of issues
total_issues_count <- 0

# Loop through each year and file prefix to import files and check for parsing issues
for (year in year_range) {
  for (prefix in file_prefixes) {
    file_path <- paste0(prefix, year, ".csv")  # Construct the file name
    full_path <- file.path(raw_data, as.character(year), file_path)  # Full file path

    # Check if the file exists, import it, and collect parsing issues
    if (file.exists(full_path)) {
      # Read the data
      test_df <- read_csv(full_path, show_col_types = TRUE)
      
      # Check for parsing problems
      parsing_issues <- problems(test_df)
      issue_count <- nrow(parsing_issues)  # Count the number of issues
      if (issue_count > 0) {
        # Store parsing issues in the list with the file path as the name
        all_parsing_issues[[paste0(prefix, year)]] <- parsing_issues
        # Add to total issues count
        total_issues_count <- total_issues_count + issue_count
      }
    } else {
      message(paste("File does not exist at:", full_path))
    }
  }
}

# Function to print all parsing issues
print_all_parsing_issues <- function(all_issues) {
  if (length(all_issues) > 0) {
    for (name in names(all_issues)) {
      issue_count <- nrow(all_issues[[name]])
      cat("\nParsing issues for file:", name, " - Total issues:", issue_count, "\n")
      print(all_issues[[name]])
    }
    cat("\nTotal number of parsing issues across all files:", total_issues_count, "\n")
  } else {
    message("No parsing issues detected for any files.")
  }
}

# Print all parsing issues for review
print_all_parsing_issues(all_parsing_issues)

```

## Clean up each of the base tables: contact, contactsource, contacttype, haentity, haentitycontact

```{r clean contact}
# Update the contact cleanup code to remove extensions (but leave incomplete phone number), and then remove hyphens from other numbers. Imputing phone number tricky, do not do it here
contact <- contact %>% 
  rename(contact_uniqueid = uniqueid) %>% 
  mutate(
    year = as.numeric(year),  # Convert year to numeric
    # Check if phone number is in the format "#######-###" (not "###-###-####") using regex and handle NAs
    has_extension_format = ifelse(!is.na(phone) & grepl("^\\d{6,}-\\d+$", phone), TRUE, FALSE),
    # Extract the digits after the hyphen in the phone column, if it exists
    extracted_extension = ifelse(has_extension_format, sub(".*-(\\d+)$", "\\1", phone), NA),
    # Compare the extracted extension with the extension column and flag matches
    is_correct_extension = ifelse(!is.na(extracted_extension) & extracted_extension == ext, TRUE, FALSE),
    # If the format matches and the extension is correct, clean up the phone number by removing the hyphen and extension part
    phone = ifelse(is_correct_extension, sub("-(\\d+)$", "", phone), phone),
    # Standardize phone number by removing all non-digit characters (e.g., hyphens, parentheses)
    phone = gsub("[^0-9]", "", phone)
  ) %>%
  # Remove temporary columns
  select(-has_extension_format, -extracted_extension, -is_correct_extension)
```

```{r clean contactsource}
contactsource <- contactsource %>% 
  rename(title_standardized = name) %>% 
  mutate(year = as.numeric(year))

#The contact source files map the numeric job id (e.g. 219) to standardized job titles (e.g. CEO). Each contact has their own free text title (e.g. "CEO and Interim CFO") but they must always map to standardized job ids (e.g. 219 and 221).
#However, there's a new crosswalk every year, and sometimes the titles change slightly. e.g. CEO became CEO: Chief Executive Officer in 2015. 
#To standardize further, take the max title for a given id. 
#Reviewed every entry to confirm that the id never switched. ie. never went from "Patient Care coordinator" to "CEO". It did not. Only changes were small, and defintely described the same job role. 

contactsource <- contactsource %>% 
  group_by(contactsourceid) %>%
  filter(year == max(year)) %>%
  ungroup() %>% 
  select(-year)

#Create is_c_suite flag
# IDs for c suite:219 - CEO: Chief Executive Officer, 221 - CFO: Chief Financial Officer, 217 - Chief Compliance Officer, 294 - Chief Experience/Patient Engagement Officer, 279 - Chief Medical Information Officer, 282 - Chief Medical Officer, 214 - Chief Nursing Head, 220 - CIO: Chief Information Officer, 289 - CNIS: Chief Nursing Informatics Officer, 224 - COO: Chief Operating Officer, 229 - CSIO/IT Security Officer, 

contactsource <- contactsource %>%
  mutate(c_suite = ifelse(contactsourceid %in% c(219, 221, 217, 294, 279, 282, 214, 220, 289, 224, 229), 1, 0))

combined_contacts <- haentitycontact %>%
  left_join(contactsource, by = c("hacontactsourceid" = "contactsourceid")) %>% 
  left_join(contact,by = c("contactid","year")) %>% 
  left_join(contacttype, by = c("typeid", "year"))

#Note: some contacts have an additional entry that represents their role on the "Steering committee". These entries in early years seem to not be associated with a contactsourceid, leading to null job titles for those obs. e.g. contact id 715247, they have three entries in 2005. 2 have the same contactsourceid (one for the parent hosp and one for the child), and then 1 entry for his role on the steering committe, with no contactsourceid.

```

```{r clean contacttype}
contacttype <- contacttype %>% 
  rename(job_category = name) %>% 
  mutate(year = as.numeric(year))
```

```{r clean haentity}

haentity <- haentity %>% 
  rename_with(tolower) %>% 
  rename(himss_entityid = haentityid,
         entity_type = haentitytype,
         entity_name = name,
         entity_address = address1,
         entity_city = city,
         entity_state = state,
         entity_zip = zip,
         entity_profitstatus = profitstatus,
         entity_parentid = parentid,
         entity_uniqueid = uniqueid,
         entity_bedsize = nofbeds,
         entity_phone = phone,
         entity_email = email
      ) %>% 
  mutate(entity_type = case_when(
                        entity_type == "IDS" ~ "IDS/RHA",
                        entity_type == "Independent Hospital" ~ "Single Hospital Health System", 
                        TRUE ~ entity_type  # Keep all other values as they are
                        )
  )

haentity <- haentity %>%
  mutate(system_id = case_when(
                      himss_entityid %in% entity_parentid ~ entity_uniqueid,
                      entity_parentid %in% himss_entityid ~ 
                        entity_uniqueid[match(entity_parentid, himss_entityid)],
                      .default = NA
                      ),
        year = as.numeric(year),
        entity_phone = gsub("[^0-9]", "", entity_phone),
        fax = gsub("[^0-9]", "", fax) 
         )

#Remove non-US entities
haentity <- haentity %>% 
    filter(!grepl("[A-Za-z]", entity_zip) & 
           !str_detect(entity_state, "AB|BC|PE|NB|NL|NS|MB|SK|PR"))

#Random Qa
haentity %>% filter(system_id ==39438)
#Interestingly, it seems the rebuild has resulted in a more complete dataset. HC Watkins was one of the systems flagged as child-no-parent, but both show up correctly. For some reason the parent hospital is missing in 2005 in the healtchare_execs file, but both records are here just fine. Not sure why

```

```{r clean haentitycontact}
haentitycontact <- haentitycontact %>% 
  rename(himss_entityid = haentityid) %>% 
  mutate(year = as.numeric(year))

```

## Targeted fixes and enhancements

```{r pull in AHA data}
# See AHA Crosswalk Comparison Code for a deeper dive into the AHA data, the completeness of the Medicare Number in the HIMSS data, and how this crosswalk was selected.

#### Pull in AHA<>MCR Crosswalk 
# set file path using manual input at beginning of script
file_path <- paste0(supplemental_data,"/",file_name_aha_mcr_xwalk)

aha_xwalk <- read_dta(file_path) %>%
  rename(ahanumber = ahaid,
         medicarenumber = mcrnum) %>%
  select(ahanumber, medicarenumber, year) %>%
  unique()

aha_xwalk <- aha_xwalk %>% 
  group_by(year, medicarenumber) %>%
  #sometimes 2 AHA numbers per MCR number, select one
  summarize(ahanumber = max(ahanumber, na.rm = TRUE), .groups = 'drop')

#### load AHA data

# set file path using manual input at beginning of script
file_path <- paste0(supplemental_data,"/",file_name_aha)

# Import the .dta file
aha_data <- read_dta(file_path) %>% 
  select(all_of(columns_to_keep)) 

aha_data <- aha_data %>% 
  rename_all(tolower) %>% 
  rename(ahanumber = id)

# Use crosswalk to pull in AHA numbers for hospitals
# Have to separate NAs, they mess up the join
haentity_na <- haentity %>%
  filter(is.na(medicarenumber))

# Handle the rows with non-NA medicarenumber
haentity_not_na <- haentity %>%
  filter(!is.na(medicarenumber)) %>%
  left_join(aha_xwalk, by = c("medicarenumber", "year")) %>%
  mutate(ahanumber = coalesce(ahanumber.y, ahanumber.x)) %>%
  select(-ahanumber.x, -ahanumber.y)

# Join the aha_data for rows with non-NA ahanumber
haentity_not_na <- haentity_not_na %>%
  left_join(aha_data, by = c("ahanumber", "year"))

# Combine the datasets back together
haentity <- bind_rows(haentity_not_na, haentity_na)

#recheck completeness
xwalk1_completeness <- haentity %>% 
  # Single Hospital Health System and IDS/RHA don't have this field completed, ever
  # filter(entity_type %in% c("Hospital", "Single Hospital Health System", "IDS/RHA")) %>%
  filter(entity_type %in% c("Hospital")) %>%
  group_by(year) %>%
  summarize(
    total_entries = n(),
    aha_num_na_count = sum(is.na(ahanumber)),
    mcr_num_na_count = sum(is.na(medicarenumber))
  ) %>% 
  group_by(year) %>% 
  summarize (
    aha_num_complete_percent = (1 - aha_num_na_count / total_entries) * 100,
    mcr_num_complete_percent = (1 - mcr_num_na_count / total_entries) * 100
  )

```

```{r fix swapped names}
#some contacts have an entry where their first and last name has been swapped

# Step 1: Create dataset w extra 2 columns that swap firstname for lastname, and last for first
swapped_contacts <- combined_contacts %>%
  filter(year>2008, status %in% c("Active","ACTIVE")) %>%
  select(year, contact_uniqueid, firstname, lastname) %>%
  mutate(
    firstname = str_to_lower(firstname),
    lastname = str_to_lower(lastname),
    swapped_firstname = lastname,
    swapped_lastname = firstname
  ) %>%
  group_by(contact_uniqueid, firstname, lastname, swapped_firstname, swapped_lastname) %>%
  summarise(year_count = n_distinct(year), .groups = 'drop') %>% 
  select(contact_uniqueid, swapped_firstname, swapped_lastname, year_count) %>% 
  unique()

# Step 2: Join the original dataset with the swapped dataset on contact_uniqueid. Then, filter on instances where the firstname in the original dataset matches the swapped lastname in the swapped dataset.
swapped_cases <- combined_contacts %>%
  filter(year>2008, status %in% c("Active","ACTIVE")) %>%
  # select(year, contact_uniqueid, firstname, lastname) %>%
  select(contact_uniqueid, firstname, lastname) %>%
  unique() %>% 
  mutate(
    firstname_l = str_to_lower(firstname),
    lastname_l = str_to_lower(lastname)) %>% 
  left_join(swapped_contacts, by = "contact_uniqueid") %>%
  filter(
  firstname_l == swapped_firstname & lastname_l == swapped_lastname) %>%
  group_by(contact_uniqueid) %>%
  #now we have to pick which is wrong, since the result here will include two entries for each contact with both possible orderings of the name. Mary Smith, and Smith Mary. Select the least frequently occurring order as the "incorrect" order.
  slice_min(year_count, n = 1, with_ties = FALSE) %>%
  ungroup() %>% 
  select(contact_uniqueid, swapped_firstname, swapped_lastname) %>%
  #create fields where the names are flipped back correctly
  mutate(fix_firstname = swapped_lastname,
         fix_lastname = swapped_firstname,
         is_swapped = 1) %>%
  unique()

# Step 3: Correct the swapped names in the original dataset
combined_contacts_corrected <- combined_contacts %>%
  mutate(
    firstname_l = str_to_lower(firstname),
    lastname_l = str_to_lower(lastname)
  ) %>%
  left_join(
    swapped_cases,
    by = c("contact_uniqueid", "firstname_l" = "swapped_firstname", "lastname_l" = "swapped_lastname")
  ) %>%
  mutate(
    firstname = if_else(!is.na(is_swapped), fix_firstname, firstname),
    lastname = if_else(!is.na(is_swapped), fix_lastname, lastname)
  ) %>%
  select(-fix_firstname, -fix_lastname, -firstname_l, -lastname_l,-is_swapped)

```

```{r lat long}
geo <- haentity %>% 
  select(year,entity_uniqueid,himss_entityid,system_id,medicarenumber, entity_name, entity_type, entity_address, address2, entity_city,entity_state,entity_zip,latitude,longitude, lat,long)

aha_data %>%
  group_by(year) %>%
  summarise(across(c(lat, long),
                   ~mean(!is.na(.)) * 100,
                   .names = "{col}_filled_pct"))

geo %>%
  filter(entity_type %in% c("Hospital")) %>%
  group_by(year) %>%
  summarise(across(c(entity_address, latitude, longitude, lat,long),
                   ~mean(!is.na(.)) * 100,
                   .names = "{col}_filled_pct"))

#AHA data not pulling in correctly. until that join fixed, use the latitude / longitude reported in HIMSS. Create a separate table that is just addresses and lat/long, and use that to join back to main HAEentity table to backfill for prior year's addresses. 

geo <- haentity %>% 
  filter(!is.na(latitude)) %>% 
  select(entity_address, address2, entity_city, entity_state, entity_zip, latitude, longitude) %>% 
  unique()

geo_avg <- haentity %>%
  #don't include entries where lat/long was not filled in
  filter(!is.na(latitude)) %>% 
  select(entity_address, entity_city, entity_state, entity_zip, latitude, longitude) %>% 
  unique() %>% 
  group_by(entity_address, entity_city, entity_state, entity_zip) %>%
  #this field is stored as character, convert to numeric so we can take the mean
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  ) %>% 
  #over the years, the precision of the lat/long changed. take the average
  summarise(
    latitude = mean(latitude, na.rm = TRUE),
    longitude = mean(longitude, na.rm = TRUE)
    ) 

haentity_filled <- haentity %>%
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  ) %>% 
  left_join(geo_avg, by = c("entity_address", "entity_city", "entity_state", "entity_zip")) %>%
  mutate(
    latitude = coalesce(latitude.x, latitude.y),
    longitude = coalesce(longitude.x, longitude.y)
  ) %>%
  select(-latitude.x, -latitude.y, -longitude.x, -longitude.y)

haentity_filled %>% 
  group_by(year) %>%
  summarise(across(c(entity_address, latitude, longitude),
                   ~mean(!is.na(.)) * 100,
                   .names = "{col}_filled_pct"))

check <- haentity_filled %>% 
  select(entity_address, entity_city, entity_state, entity_zip, latitude, longitude) 

```
```{r gender}
# consider using 80% frequency threshold for salutation frequency, since that's what katherine does later for metaphones
#add gender to combined contacts corrected
# uses modified assign_genders.Rmd code

# combined_contacts_corrected <- combined_contacts_corrected %>% 
#   mutate(full_name = str_to_lower(str_replace_all(paste0(firstname, lastname), 
#                                                   "[[:punct:]\\s]", ""))) 
# 
# ############################  PART 1 - SALUTATIONS ############################
# ### - use salutations from data that are consistent at the person level and ###
# ### at the level of the firstname                                           ###
# ###############################################################################
# case1 <- combined_contacts_corrected %>%
#   group_by(full_name) %>%
#   filter(n_distinct(salutation) == 1 & !any(is.na(salutation))) %>%
#   ungroup() %>%
#   select(full_name, firstname, lastname, salutation) %>%
#   distinct(firstname, salutation) %>%
#   group_by(firstname) %>%
#   mutate(count = n()) %>%
#   ungroup()
# 
# confirmed_1 <- case1 %>% # confirmed 10,301 genders
#     filter(count==1 & (salutation != "Sr." & salutation != "Fr."))
# 
# confirmed_names <- confirmed_1$firstname
# remaining_1 <- combined_contacts_corrected %>%
#   filter(!firstname %in% confirmed_names) # leaving 5787 genders
# 
# ###############################  PART 2 - WGND ###############################
# ### - use wgnd us codes to determine gender                                ###
# ################################################################################
# wgnd2 <- read.csv(paste0(user_directory, "/nicknames dictionaries/wgnd_2.csv"))
# us <- wgnd2 %>% 
#   filter(code == "US")
# 
# unique_names <- remaining_1 %>%
#   distinct(firstname) %>%
#   mutate(name = tolower(firstname))
# 
# merged_df <- left_join(unique_names, us, by = "name")
# remaining_2 <- merged_df %>%
#   filter(is.na(gender))
# confirmed_2 <- bind_rows(confirmed_1, merged_df %>%filter(!is.na(gender))) %>%
#   mutate(gender = ifelse(is.na(gender) & salutation == "Mr.", "M",
#                          ifelse(is.na(gender) & salutation == "Ms.", "F", 
#                                 ifelse(!is.na(gender), gender, NA))))

```

## Compile final complete entity+contact table

```{r final join to create table}
entity_contacts <- combined_contacts_corrected %>% 
  left_join(haentity, by = c("himss_entityid", "year"))
```

```{r export to dropbox}
# Export compiled dataset
file_name <- "himss_entities_contacts_0517_v1"
file_path_feather <- paste0(user_directory,project_directory,"/derived/",file_name,".feather")
write_feather(entity_contacts, file_path_feather)

#export just compiled entity info
file_name <- "himss_entities_only_0517"
file_path_feather <- paste0(user_directory,project_directory,"/derived/",file_name,".feather")
write_feather(haentity, file_path_feather)

```
